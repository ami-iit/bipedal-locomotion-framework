{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# velMANNTrajectoryGenerator\n",
    "This notebook contains the code showing how to use velMANNTrajectoryGenerator to generate a walking trajectory for the ergoCub humanoid robot. \n",
    "\n",
    "If you are interested in some details of the algorithm, please refer to the paper: [Trajectory Generation with Physics-Informed Learning and Drift Mitigation](https://github.com/ami-iit/paper_delia_2024_ral_physics-informed_trajectory_generation).\n",
    "\n",
    "Differently from the `velMANNAutoregressive`, `velMANNTrajectoryGenerator` class can be used to plan a trajectory that can be fed into an MPC. Indeed it is possible to sample the contact and extract a `ContactPhaseList` that can be used to initialize the MPC. Moreover the `velMANNTrajectoryGenerator` can be reset to any point of the trajectory, allowing to replan at will.\n",
    "\n",
    "## Import all the required packages\n",
    "In this section we import all the required packages. In particular, we need `iDynTree` to correctly visualize the robot, `numpy` to perform some basic operations, `manifpy` to perform some basic operations on manifolds, `resolve_robotics_uri_py` to correctly locate the `ergoCub` model. Finally `bipedal_locomotion_framework` implements the MANN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e357295-6730-47d6-9119-bd09ed598e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from idyntree.visualize import MeshcatVisualizer\n",
    "import idyntree.bindings as idyn\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import manifpy as manif\n",
    "import bipedal_locomotion_framework as blf\n",
    "import resolve_robotics_uri_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the ingredients\n",
    "To correctly run the notebook, we need to prepare the ingredients. In particular, we need to:\n",
    "- Get the network model: In this case we use the model trained on the ergoCub robot. The model is available [here](https://huggingface.co/ami-iit/mann/resolve/main/ergocubSN000_26j_49e_velbased_mann.onnx).\n",
    "- Load the configuration file: The configuration file contains all the parameters needed to correctly run the network and generate a proper set of inputs for the network. In detail we have two configuration files:\n",
    "    - `config_mann.toml`: This file contains the parameters needed to correctly load and run the network.\n",
    "    - `config_joypad.toml`: This file contains the parameters needed to correctly generate the input for the network. In particular, it contains the parameters needed to correctly map a joypad input to the network input.\n",
    "- Load the robot model: In this case we use the `ergoCub` model. We load the model only to correctly visualize the robot.\n",
    "\n",
    "### Get the network model\n",
    "What we need to do is to download the network model and save it in the `config` folder. The model is available [here](https://huggingface.co/ami-iit/mann/resolve/main/ergocubSN000_26j_49e_velbased_mann.onnx). \n",
    "Moreover we load the parameters needed to correctly run the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08029010-6814-4101-af0f-ac2940560eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use urllib to download the onnx model from huggingface\n",
    "import urllib.request\n",
    "import os\n",
    "url = \"https://huggingface.co/ami-iit/mann/resolve/main/ergocubSN000_26j_49e_velbased_mann.onnx\"\n",
    "urllib.request.urlretrieve(url, \"./config/ergocubSN000_26j_49e.onnx\")\n",
    "\n",
    "# Get the configuration files\n",
    "config_path = Path(\"__file__\").parent / \"config\" / \"config_mann.toml\"\n",
    "params_network = blf.parameters_handler.TomlParametersHandler()\n",
    "params_network.set_from_file(str(config_path))\n",
    "\n",
    "joypad_config_path = Path(\"__file__\").parent / \"config\" / \"config_joypad.toml\"\n",
    "params_joypad = blf.parameters_handler.TomlParametersHandler()\n",
    "params_joypad.set_from_file(str(joypad_config_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ergoCub model\n",
    "We load the `ergoCub` model to correctly visualize the robot.\n",
    "The model is loaded using `iDynTree` and `resolve_robotics_uri_py` to correctly locate the model. Please notice that the model is loaded specifying the same joint order used to train the network.\n",
    "Moreover we load a set of boxes to correctly visualize the contact points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3817e56-9e8d-464e-9f0b-6577a2c0f2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path of the robot model\n",
    "robot_model_path = str(resolve_robotics_uri_py.resolve_robotics_uri(\"package://ergoCub/robots/ergoCubSN001/model.urdf\"))\n",
    "ml = idyn.ModelLoader()\n",
    "ml.loadReducedModelFromFile(robot_model_path, params_network.get_parameter_vector_string(\"joints_list\"))\n",
    "viz = MeshcatVisualizer()\n",
    "viz.load_model(ml.model(), \"real\", 0.3)\n",
    "viz.load_model(ml.model(), \"ghost\", 0.8)\n",
    "viz.load_box(0.2, 0.1, 0.001, \"left1\", [219/255.0, 68/255.0, 55/255.0, 1])\n",
    "viz.load_box(0.2, 0.1, 0.001, \"left2\", [219/255.0, 68/255.0, 55/255.0, 1])\n",
    "viz.load_box(0.2, 0.1, 0.001, \"right1\", [219/255.0, 68/255.0, 55/255.0, 1])\n",
    "viz.load_box(0.2, 0.1, 0.001, \"right2\", [219/255.0, 68/255.0, 55/255.0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network initialization\n",
    "In this section we initialize the network. In particular we first instantiate the network and then we load the parameters. The parameters are loaded from the `config_mann.toml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the trajectory generator\n",
    "mann_trajectory_generator = blf.ml.velMANNTrajectoryGenerator()\n",
    "assert mann_trajectory_generator.set_robot_model(ml.model())\n",
    "assert mann_trajectory_generator.initialize(params_network)\n",
    "\n",
    "# Create the input builder\n",
    "input_builder = blf.ml.velMANNAutoregressiveInputBuilder()\n",
    "assert input_builder.initialize(params_joypad)\n",
    "\n",
    "# Create the input\n",
    "mann_trajectory_generator_input = blf.ml.velMANNTrajectoryGeneratorInput()\n",
    "mann_trajectory_generator_input.merge_point_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to reset the network to the initial state. In particular, we need to reset  the joint state and the base pose in a given configuration already seen by the network during traning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial joint positions configuration. The serialization is specified in the config file\n",
    "joint_positions = params_network.get_parameter_vector_float(\"initial_joints_configuration\")\n",
    "\n",
    "# Initial base pose. This pose makes the robot stand on the ground with the feet flat\n",
    "base_pose = manif.SE3.Identity()\n",
    "initial_base_height = params_network.get_parameter_float(\"initial_base_height\")\n",
    "quat = params_network.get_parameter_vector_float(\"initial_base_quaternion\")\n",
    "quat = quat / np.linalg.norm(quat) # Normalize the quaternion\n",
    "base_pose = manif.SE3([0, 0, initial_base_height], quat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally ask the input builder to generate an input that mimics a joypad input. In particular, we ask the input builder to generate an input that mimics a joypad input corresponding to a forward walking, i.e. a joypad input with the axes moved to the following values:\n",
    "- `left_stick_x`: 1.0\n",
    "- `left_stick_y`: 0.0\n",
    "- `right_stick_x`: 1.0\n",
    "- `right_stick_y`: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_builder_input = blf.ml.velMANNDirectionalInput()\n",
    "input_builder_input.motion_direction = np.array([1, 0])\n",
    "input_builder_input.base_direction = np.array([1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network execution\n",
    "We now visualize the robot in the viewer and we run the network for 50 steps. In particular, we run the trajectory generator for 50 steps and we visualize the result in the viewer. For each step we show:\n",
    "- The entire future trajectory of the robot.\n",
    "- The sampled contacts.\n",
    "- The robot at the merge point. The merge point is the point from which the network starts to generate the trajectory at the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.jupyter_cell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the trajectory generator\n",
    "far_position = np.array([1e5, 1e5, 1e5])\n",
    "identity = np.eye(3)\n",
    "mann_trajectory_generator.set_initial_state(joint_positions, base_pose)\n",
    "\n",
    "for _ in range(50):\n",
    "\n",
    "    # Set the input to the builder and\n",
    "    input_builder.set_input(input_builder_input)\n",
    "    assert input_builder.advance()\n",
    "    assert input_builder.is_output_valid()\n",
    "\n",
    "    mann_trajectory_generator_input.desired_future_base_trajectory = input_builder.get_output().desired_future_base_trajectory\n",
    "    mann_trajectory_generator_input.desired_future_base_velocities = input_builder.get_output().desired_future_base_velocities\n",
    "    mann_trajectory_generator_input.desired_future_base_directions = input_builder.get_output().desired_future_base_directions\n",
    "\n",
    "    # Set the input to the trajectory generator\n",
    "    mann_trajectory_generator.set_input(mann_trajectory_generator_input)\n",
    "    assert mann_trajectory_generator.advance()\n",
    "    assert mann_trajectory_generator.is_output_valid()\n",
    "\n",
    "    # Get the output of the trajectory generator and update the visualization\n",
    "    mann_output = mann_trajectory_generator.get_output()\n",
    "\n",
    "    # Move the feet to a far position\n",
    "    for i in range(1,3):\n",
    "        viz.set_primitive_geometry_transform(far_position, identity, \"left\" + str(i))\n",
    "        viz.set_primitive_geometry_transform(far_position, identity, \"right\" + str(i))\n",
    "\n",
    "    # Show the contact sampled by the trajectory generator\n",
    "    contact_map = mann_output.phase_list.lists()\n",
    "    i = 1\n",
    "    for contact in contact_map[\"left_foot\"]:\n",
    "        viz.set_primitive_geometry_transform(contact.pose.translation(), contact.pose.rotation(), \"left\" + str(i))\n",
    "        i += 1\n",
    "\n",
    "    i = 1\n",
    "    for contact in contact_map[\"right_foot\"]:\n",
    "        viz.set_primitive_geometry_transform(contact.pose.translation(), contact.pose.rotation(), \"right\" + str(i))\n",
    "        i += 1\n",
    "\n",
    "    # Show the trajectory\n",
    "    for i in range(len(mann_output.joint_positions)):\n",
    "        viz.set_multibody_system_state(mann_output.base_poses[i].translation(),\n",
    "                                       mann_output.base_poses[i].rotation(),\n",
    "                                       mann_output.joint_positions[i],\n",
    "                                       \"real\")\n",
    "\n",
    "        # Show the robot at the merge point\n",
    "        if i == mann_trajectory_generator_input.merge_point_index:\n",
    "            viz.set_multibody_system_state(mann_output.base_poses[i].translation(),\n",
    "                                           mann_output.base_poses[i].rotation(),\n",
    "                                           mann_output.joint_positions[i],\n",
    "                                           \"ghost\")\n",
    "\n",
    "    mann_trajectory_generator_input.merge_point_index = 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
